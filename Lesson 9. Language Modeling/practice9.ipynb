{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Взять набор данных на ваше усмотрение (стихи/прозу) или что-то ещё для примера можно так же использовать прикреплённый Евгений Онегин\n",
    "\n",
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Александр Сергеевич Пушкин\n",
      "\n",
      "                                Евгений Онегин\n",
      "                                Роман в стихах\n",
      "\n",
      "                        Не мысля гордый свет забавить,\n",
      "                        Вниманье дружбы возлюбя,\n",
      "                       \n"
     ]
    }
   ],
   "source": [
    "PATH_TO_FILE = '../data/evgenyi_onegin.txt'\n",
    "\n",
    "with open(file=PATH_TO_FILE, mode='r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. поэкспериментировать с посимвольным подходом\n",
    "\n",
    "## Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "idx2char = np.array(vocab)\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(101, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tInput data:  'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                         '\n",
      "\n",
      "\tTarget data: 'лександр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('\\tInput data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('\\n\\tTarget data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10_000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoint/cp.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded\n",
      "Epoch 1/5\n",
      "44/44 [==============================] - 59s 1s/step - loss: 1.8788\n",
      "\n",
      "Epoch 00001: saving model to checkpoint\\cp.ckpt\n",
      "Epoch 2/5\n",
      "44/44 [==============================] - 53s 1s/step - loss: 1.8849\n",
      "\n",
      "Epoch 00002: saving model to checkpoint\\cp.ckpt\n",
      "Epoch 3/5\n",
      "44/44 [==============================] - 52s 1s/step - loss: 1.8720\n",
      "\n",
      "Epoch 00003: saving model to checkpoint\\cp.ckpt\n",
      "Epoch 4/5\n",
      "44/44 [==============================] - 54s 1s/step - loss: 1.8609\n",
      "\n",
      "Epoch 00004: saving model to checkpoint\\cp.ckpt\n",
      "Epoch 5/5\n",
      "44/44 [==============================] - 51s 1s/step - loss: 1.8436\n",
      "\n",
      "Epoch 00005: saving model to checkpoint\\cp.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='sparse_categorical_crossentropy'\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_weights(checkpoint_path)\n",
    "    print('checkpoint loaded')\n",
    "except Exception:\n",
    "    print('checkpoint not found')\n",
    "\n",
    "if True:\n",
    "    history = model.fit(\n",
    "        dataset, \n",
    "        epochs=5,\n",
    "        callbacks=[cp_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=1\n",
    ")\n",
    "model.load_weights(checkpoint_path)\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate = 500, temperature=1):\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        \n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "привет\n",
      "      N\n",
      "fXЭLю\n",
      "av\n",
      "d28    жанц\n",
      "p8с3ьhDФрц\n",
      "Но\n",
      "          (Е.\n",
      "   саен \n",
      ")\n",
      "ъRАЖэ\n",
      "\n",
      "ю родц\n",
      "\n",
      "tъ8W\"йэ\n",
      "\n",
      "Цэ(\n",
      "SXил сн,\n",
      "\n",
      " HФа\n",
      "влон\n",
      "\n",
      ")AжoP,\n",
      "muс олше\n",
      "\n",
      "  ы    ц\"\n",
      "N  пъчшш\n",
      "ъя\n",
      "\n",
      "     m\n",
      "yМлнш\n",
      "Глpн.\n",
      "\n",
      "  SХлено\n",
      "        X1ун\n",
      "\n",
      " ялA\n",
      "\n",
      "   ГлъфшЛе\n",
      "\n",
      "  IXФо\n",
      "\n",
      "r  1z}\n",
      "SI\n",
      "  клац\n",
      "   vs\n",
      "uМл\n",
      "GЗлжеее\n",
      "\n",
      "  u\"бя,\n",
      "\n",
      "ЗDЬ2\n",
      "Q-rrieq\n",
      "ЬЗГчшчшл.\n",
      "      7pЗА\n",
      "\n",
      "Hw-dИ  зos0,\n",
      "   {Ли\n",
      "\n",
      "    MУншег\n",
      "R4lцiАА\n",
      "0Жнь.\n",
      "\n",
      "2}\n",
      "   YПалсео\n",
      "\n",
      "'q'Ян,\n",
      "\n",
      "L8r?  зоТ\n",
      "RЖаее\n",
      "\n",
      "8XaЗЭ2MВлwш\n",
      "\n",
      "   к\n",
      "Vут\n",
      "\n",
      "nmOB\n",
      "ъБлuyб\n",
      "dБлБp)\n",
      "qю\n",
      "\n",
      "1Gmюн5Ф,\n",
      "      ра\n",
      "\n",
      "  Фа\n",
      "          Гл1gЦ    k747.\n",
      "  чшЯ\n",
      "\n",
      "ORo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"привет\", temperature=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод:\n",
    "- обучение занимает очень много времени на ЦП\n",
    "- этот подход кажется более перспективным, так как он жестко не привязан к наличию данных в датасете и модель получается более гибкой\n",
    "\n",
    "# 2. проверить насколько изменится качество генерации текста при токенизации по словам\n",
    "\n",
    "## Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "word_vocab = sorted(set(word_tokens))\n",
    "\n",
    "idx2word = np.array(word_vocab)\n",
    "word2idx = {w: i for i, w in enumerate(word_vocab)}\n",
    "\n",
    "text_as_int = np.array([word2idx[w] for w in word_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tInput data:  'Александр Сергеевич Пушкин Евгений Онегин Роман в стихах Не мысля гордый свет забавить , Вниманье дружбы возлюбя , Хотел бы я тебе представить Залог достойнее тебя , Достойнее души прекрасной , Святой исполненной мечты , Поэзии живой и ясной , Высоких дум и простоты ; Но так и быть - рукой пристрастной Прими собранье пестрых глав , Полусмешных , полупечальных , Простонародных , идеальных , Небрежный плод моих забав , Бессонниц , легких вдохновений , Незрелых и увядших лет , Ума холодных наблюдений И сердца горестных замет . ГЛАВА ПЕРВАЯ И жить торопится и чувствовать спешит . Кн . Вяземский'\n",
      "\n",
      "\tTarget data: 'Сергеевич Пушкин Евгений Онегин Роман в стихах Не мысля гордый свет забавить , Вниманье дружбы возлюбя , Хотел бы я тебе представить Залог достойнее тебя , Достойнее души прекрасной , Святой исполненной мечты , Поэзии живой и ясной , Высоких дум и простоты ; Но так и быть - рукой пристрастной Прими собранье пестрых глав , Полусмешных , полупечальных , Простонародных , идеальных , Небрежный плод моих забав , Бессонниц , легких вдохновений , Незрелых и увядших лет , Ума холодных наблюдений И сердца горестных замет . ГЛАВА ПЕРВАЯ И жить торопится и чувствовать спешит . Кн . Вяземский .'\n"
     ]
    }
   ],
   "source": [
    "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = word_dataset.batch(101, drop_remainder=True)\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('\\tInput data: ', repr(' '.join(idx2word[input_example.numpy()])))\n",
    "    print('\\n\\tTarget data:', repr(' '.join(idx2word[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_checkpoint_path = 'word_checkpoint/cp.ckpt'\n",
    "word_checkpoint_dir = os.path.dirname(word_checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=word_checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "checkpoint loaded\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 17s 4s/step - loss: 6.4165\n",
      "\n",
      "Epoch 00001: saving model to word_checkpoint\\cp.ckpt\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.6200\n",
      "\n",
      "Epoch 00002: saving model to word_checkpoint\\cp.ckpt\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 18s 4s/step - loss: 7.9895\n",
      "\n",
      "Epoch 00003: saving model to word_checkpoint\\cp.ckpt\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 16s 4s/step - loss: 8.2109\n",
      "\n",
      "Epoch 00004: saving model to word_checkpoint\\cp.ckpt\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 16s 4s/step - loss: 7.8681\n",
      "\n",
      "Epoch 00005: saving model to word_checkpoint\\cp.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(word_vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='sparse_categorical_crossentropy'\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_weights(word_checkpoint_path)\n",
    "    print('checkpoint loaded')\n",
    "except Exception:\n",
    "    print('checkpoint not found')\n",
    "\n",
    "if True:\n",
    "    history = model.fit(\n",
    "        dataset, \n",
    "        epochs=5,\n",
    "        callbacks=[cp_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(word_vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=1\n",
    ")\n",
    "model.load_weights(word_checkpoint_path)\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate = 500, temperature=1):\n",
    "    input_eval = [word2idx[s] for s in [start_string]]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        \n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2word[predicted_id])\n",
    "\n",
    "    return (start_string + ' '.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пушкинпомеркшею заморозил шее Жуковский начал осуждал цветет тайно Постели тайною страницы Сорочка согнала луч глупцы откуда лондонский блаженства Предтеча ласкаться окроплю живо бросают Боится клевещут ждут что-то убрана туда мельницу Благоразумной Любить Бульвары Знаком начнет шутке б отдалась тюрьмы послан дожде драки согласно всечасно сению вести пародия Хоть Вотще наводит обычный нею Замолкло эпиграмме целовать ревнивом берегов обращает волны слушает кружок Тобою благородство Затея упований шуток речкою явился вылитым поэты снимок медалью отвела костяные благоразумных высоко из Сердечной слабости псарне потонет одежд дрожало толпа Забудет ж страх тишины Харитонья деве Петушкову теша изображу мадригал обратя гусары Филипьевна грустить свечах непременно\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"Пушкин\", temperature=1, num_generate = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод:\n",
    "- при обучении модели функция потерь показывает очень большой показатель\n",
    "- слова в датасете уже заготовлены, так что общаться с моделью приходится исходя из содержания датасета, что делает модель менее гибкой чем при предыдущем подходе\n",
    "- данный подход позволяет пропустить этап формирования слов, что ускоряет процесс генерации текста"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
